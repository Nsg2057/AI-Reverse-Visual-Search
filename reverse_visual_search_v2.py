# -*- coding: utf-8 -*-
"""Reverse Visual Search - v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dN1pSxXHxBtMpUJYRQllVJ6e0cVT0W33
"""

import tensorflow as tf
import tensorflow_hub as thub
import sklearn as sk
from tensorflow.keras import Sequential
from sklearn.datasets import fetch_lfw_people
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
import numpy as np
import cv2
from tensorflow.keras.preprocessing import image
from numpy.linalg import norm
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input as inception_preprocess
from keras.models import load_model
from sklearn.neighbors import KNeighborsClassifier as KNN

from matplotlib import pyplot as plt

from google.colab.patches import cv2_imshow
import PIL

# mount google drive
from google.colab import drive
drive.mount('/content/drive')

proj_root = 'drive/MyDrive/AI-FinalProject/'

'''
import tarfile
filename = proj_root + 'lfw.tgz'
tar = tarfile.open(filename, "r:gz")
tar.extractall(path=proj_root)
tar.close()
'''

data_path = proj_root + "lfw"
lfw_people = fetch_lfw_people(funneled = False, resize=1, min_faces_per_person=5, color=True)
#n_samps, height, width, color(rgb)
#images should have shape (height,weight,color)

#extract images
X = lfw_people.images

#extract image label list
y = lfw_people.target

#corresponding label list [int -> str]
labels = lfw_people.target_names

#sanity check
print("Num Pictures: ", len(X))
print("Label Vector: ", y)
print("Number of People: ", len(labels))

#random seed to regenerate randomness in other section
X = X.astype(np.int32)

#this is when we're testing accuracy
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
#scaler = StandardScaler()
#X_train = scaler.fit_transform(X_train)
#X_test = scaler.transform(X_test)
test_img = X_train[0]
#test_img = X[0]
plt.imshow(test_img)

res_model = tf.keras.applications.resnet50.ResNet50(weights='imagenet', include_top=False, input_shape = test_img.shape, pooling='max')
#res_model.summary()

"""**Warning: This model feature extraction takes a long time**"""

from tqdm import tqdm
def base_feature_extraction(x_train, x_test, model_type = 'res_model', include_test = True):
  '''
  Uses ResNet model to extract features for knn
  '''
  if model_type == 'res_model':
    res_model = tf.keras.applications.resnet50.ResNet50(weights='imagenet', include_top=False, input_shape = (150,150,3), pooling='max')
  else:
    return
  train_points = list()
  test_points = list()

  for image in tqdm(x_train):
    dim= (150,150)
    input = cv2.resize(image.astype('uint8'), dim, interpolation=cv2.INTER_AREA)
    input = np.expand_dims(input, axis=0)
    input = preprocess_input(input)
    train_point = res_model(input, training=False) #should be 1 x 2048
    train_point_norm = train_point / norm(train_point)
    train_points.append(train_point_norm[0])

  if include_test:
    for image in tqdm(x_test):
      dim= (150,150)
      input = cv2.resize(image.astype('uint8'), dim, interpolation=cv2.INTER_AREA)
      input = np.expand_dims(input, axis=0)
      input = preprocess_input(input)
      test_point = res_model(input, training=False)
      test_point_norm = test_point / norm(test_point)
      test_points.append(test_point_norm[0])
  
  return train_points, test_points

'''
points = list()
for image in tqdm(X_train):
  input = np.expand_dims(image, axis=0)
  input = preprocess_input(input)
  point = res_model.predict(input) #should be 1 x 2048
  point_norm = point / norm(point)
  points.append(point_norm[0])
  '''
#we're going to try to extract feature vectors from all the images, so we'll just use X
train_points, test_points = base_feature_extraction(X, x_test = [], include_test = False)

knn = KNN(n_neighbors=20, algorithm='brute')
knn.fit(train_points, y)
#y_pred = knn.predict(test_points)

#get accuracy score using accuracy metric from sklearn.metrics
#TODO document how accuracy metric works/loss function?
acc = accuracy_score(y_test, y_pred)
print(acc) #31.3% accuracy

"""Now to handle any random input and return the 10 closest images.

We'll assume that the knn model is trained already
"""

def knn_closest_images(image, knn_model, model_type='resnet'):

  '''
  Params:
  image - input
  KNN_model - KNN cluster trained on the model_type model
  model_type - used for preprocessing for specific model types - facenet (Inception) and resnet (ResNet) supported
  model - Keras Model
  '''
  if model_type == 'resnet':
    #dim= (125,94)
    #input = cv2.resize(image.astype('uint8'), dim, interpolation=cv2.INTER_AREA)
    res_model = tf.keras.applications.resnet50.ResNet50(weights='imagenet', include_top=False, input_shape = image.shape, pooling='max')
    input = np.expand_dims(image, axis=0)
    input = preprocess_input(input)
    point = res_model(input, training=False)
    point_norm = point/norm(point)

  #making predictions
  indices = knn_model.kneighbors(point_norm, return_distance=False)
  return indices
  
#indices = knn_closest_images(X[151], knn_model = knn, model_type='resnet')

import glob
query_images = dict()
for file in glob.glob(proj_root+"query_faces/original_faces/*"):
  name = file[57:-4]
  pic = cv2.imread(file)
  query_images[name] = pic

for image in query_images:
  
  indices = knn_closest_images(query_images[image], knn_model = knn, model_type='resnet')
  plt.figure(figsize=(20,20))
  for n, index in enumerate(indices[0]):
    plt.subplot(4,5,n+1)
    plt.imshow(X[index])
  plt.savefig(proj_root+"query_faces/baseline/baseline_"+image+"_closest_faces.png")

original_image = X_test[151]
print("Original Image: ")
plt.figure()
plt.imshow(original_image)


print("Closest Images")
plt.figure(figsize=(20,20))
for n, index in enumerate(indices[0]):
  plt.subplot(4,5,n+1)
  plt.imshow(X[index])

"""##FaceNet Upgrade##

Here we essentially copy the same architecture as above, but use FaceNet instead of ResNet since it is trained specifically on faces, in conjunction with the kMeans++ algorithm for clustering to improve the algorithm.

Use inception_Resnet_v2 preprocessing because facenet is based on this model.
"""

from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input as inception_preprocess

face_model = load_model(proj_root + "facenet_keras.h5", compile=False)
face_model.load_weights(proj_root+"facenet_keras_weights.h5")
face_model.trainable=False
w2 = face_model.get_weights()

from keras.models import Model
#remove the final few layers of the FaceNet model
model2 = Model(face_model.input, face_model.layers[-4].output)

import cv2
test_img = X_train[0]
dim= (160,160)
test_img = cv2.resize(test_img.astype('uint8'), dim, interpolation=cv2.INTER_AREA)
test_img = np.expand_dims(test_img, axis=0)
test_img = inception_preprocess(test_img)
point = model2(test_img, training=False) 
point_norm = point/norm(point)
print(point_norm)

#exclusively for facenet
def extract_features(X,input_shape=(160,160),model=model2):
  out = list()
  for image in tqdm(X):
    input = cv2.resize(image.astype('uint16'), input_shape, interpolation = cv2.INTER_AREA)
    input = np.expand_dims(input, axis=0)
    input = inception_preprocess(input)
    point = model(input, training=False) #should be 1 x 1792
    point_norm = point/norm(point)
    out.append(point_norm[0])
  return out

#use the entire X dataset
train_points = extract_features(X,model=model2)
#extract from all images - no test set for query faces
#test_points_facenet = extract_features(X_test)

"""Lets try K Nearest Neighbors algorithm with the faceNet pretrained neural network output. Note that the dimensionality of the final output is smaller (1700 vs 2048), and the network is built to extract features from specifically faces.

So we would expect the FaceNet output to be superior to ResNet.
"""

knn_face = KNN(n_neighbors=20)
#train on the entire dataset
knn_face.fit(train_points, y)

#test set only used for accuracy training
#y_pred = knn_face.predict(test_points_facenet)
#acc = accuracy_score(y_test, y_pred)
#print("FaceNet + KNN accuracy: ", acc) 
#FaceNet+ KNN accuracy:  0.8213689482470785

"""Essentially the same code to extract closest images into a folder from above"""

import glob
query_images = dict()
for file in glob.glob(proj_root+"query_faces/original_faces/*"):
  name = file[57:-4]
  pic = plt.imread(file).astype(np.uint32)
  query_images[name] = pic
  plt.imshow(pic)

for image in query_images:
  features = extract_features([query_images[image]])
  indices = knn_face.kneighbors(features, return_distance=False)
  plt.figure(figsize=(20,20))
  for n, index in enumerate(indices[0]):
    plt.subplot(4,5,n+1)
    plt.imshow(X[index])
  plt.savefig(proj_root+"query_faces/facenetknn/facenetknn_"+image+"_closest_faces.png")

"""NuSVC - SVM based method that allows manipulation of marginal and support vector parameters"""

from sklearn.svm import SVC, NuSVC, LinearSVC
def svm_classification(method, x_train, y_train, params):
  if method=='SVC':
    svm = SVC()
  elif method=='NuSVC':
    nu = params
    svm = NuSVC(nu=nu)
  elif method=='LinearSVC':
    loss, max_iter = params
    svm = LinearSVC(loss=loss, max_iter=max_iter)
  else:
    print("Use proper SVM method for classification")
    return
  
  svm.fit(x_train, y_train)
  #y_pred = svm.predict(x_test)
  #acc = accuracy_score(y_test, y_pred)
  return svm

"""SVM classifier takes a long time if using entire dataset, maybe need to reduce slightly"""

svm = svm_classification("NuSVC", x_train = train_points,
                         y_train = y, params = 0.005)

#y_pred = svm.predict(test_points_facenet)
#acc = accuracy_score(y_test, y_pred)

#print("FaceNet + NuSVM accuracy: ", acc)
#FaceNet + SVM accuracy:  0.994991652754591

query_images = dict()
for file in glob.glob(proj_root+"query_faces/original_faces/*"):
  name = file[57:-4]
  pic = plt.imread(file)
  query_images[name] = pic

for image in query_images:
  features = extract_features([query_images[image]])
  y_pred = svm.predict(features)
  pred_img = [X[i] for i in range(len(X)) if y[i] == y_pred]
  n = 0
  plt.figure(figsize=(20,20))
  for img in pred_img:
    if n >= 20:
      break
    plt.subplot(4,5,n+1)
    plt.imshow(img)
    n+=1
  plt.savefig(proj_root+"query_faces/facenetsvm/facenetsvm_"+image+"_closest_faces.png")



from sklearn.svm import SVC, NuSVC, LinearSVC
'''
svm = LinearSVC(loss = 'hinge', max_iter=2000)
svm.fit(train_points, y_train)

y_pred = svm.predict(test_points_facenet)
print(accuracy_score(y_test,y_pred))
'''
svm = svm_classification("LinearSVC", x_train = train_points,
                         y_train = y_train, params = ('hinge', 2000))

y_pred = svm.predict(test_points_facenet)

print("Accuracy of LinearSVM prediction: ", accuracy_score(y_test, y_pred))
#Accuracy of LinearSVM prediction:  0.9833055091819699







